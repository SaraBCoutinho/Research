# -*- coding: utf-8 -*-
"""neuralTrafficFlow_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nk9dllYlv267XpfLT6g-LvNNDnbq9pBp

# IMPORTS
"""

!pip install pandas
!pip install numpy

import numpy as np 
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import preprocessing
from sklearn.model_selection import train_test_split, cross_validate, TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
from sklearn.metrics import confusion_matrix

from sklearn.linear_model import LogisticRegression, RidgeClassifierCV, RidgeClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.neighbors import KNeighborsClassifier

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import regularizers

from  IPython import display
from matplotlib import pyplot as plt

import numpy as np

import pathlib
import shutil
import tempfile

#!pip install git+https://github.com/tensorflow/docs

#import tensorflow_docs as tfdocs
#import tensorflow_docs.modeling
#import tensorflow_docs.plots

"""# DATA

* habilitar acesso para salvar no resultados no drive e definir path da planilha resultados
"""

#from google.colab import drive
#drive.mount('/content/drive')

path = '/Volumes/EXT/EXTDocumentos/UFPE/2021.1/Redes neurais/project/Projeto/Resultados.xlsx'

"""* index_col para criar multindex com data e hora
* header para criar multilevels com velocidade e equipamento
"""

X = pd.read_csv('https://www.cin.ufpe.br/~amal/dados/transito/DATAMART/inputhgeralpivot.csv', index_col=[0,1], header=[0,1])
#X = pd.read_csv('https://www.cin.ufpe.br/~amal/dados/transito/DATAMART/inputhfaixapivot.csv', index_col=[0,1], header=[0,1,2])

X.head()

X.swaplevel(axis=1).sort_index(axis=1)

X.shape

"""* index_col para criar multindex com data e hora
* header para criar multilevels com count e bairro
* drop pra apagar essa coluna 0 q não serve pra nada
* count pra ignorar os dados de qtd de vítimas, somente acidentes
"""

Xg = pd.read_csv('https://www.cin.ufpe.br/~amal/dados/transito/DATAMART/inputhgeralpivot.csv', index_col=[0,1], header=[0,1])
Xg = Xg.drop('latitude',axis=1).drop('longitude',axis=1).drop('tipo_y',axis=1)
Xg = Xg.swaplevel(axis=1).sort_index(axis=1)

Xg.sum(axis=0).sort_values()

Xg.isna().sum().sum() / Xg.isna().count().sum()

#Xg[Xg.columns[0:11]]
#Xg[Xg.columns[0:11]].sort_values(Xg.columns[5],ascending=False)
#Xg[Xg.columns[0:11]].iloc[52*24+10].plot.bar()
#Xg[Xg.columns[0:11]].iloc[348*24+4].plot.bar()

selecao_bairros = ['Areias', 'Beberibe' ,'Boa Viagem' ,'Boa Vista' ,'Cabanga', 'Casa Amarela' , 'Casa Forte' ,'Caçote' ,'Curado' ,'Derby' ,'Estância' ,'Graças' ,'Imbiribeira' ,'Iputinga' ,'Jiquiá' ,'Macaxeira' ,'Madalena' ,'Paissandu' ,'Pina' , 'Prado' ,'Bairro do Recife' ,'Santo Amaro' ,'São José' ,'Tamarineira' ,'Tejipió' ,'Torrões' ,'Várzea', 'Vasco da Gama' ]
bairros = [ b.upper() for b in selecao_bairros ]

Y = pd.read_csv('https://www.cin.ufpe.br/~amal/dados/transito/DATAMART/outputhgeralpivotbin.csv', index_col=[0,1], header=[0,1]).drop('0', level=1, axis=1)['count']

Y.shape

Yg = pd.read_csv('https://www.cin.ufpe.br/~amal/dados/transito/DATAMART/outputhgeralpivotbin.csv', index_col=[0,1], header=[0,1]).drop('0', level=1, axis=1)['count']
Yg = Yg[bairros[0:]]
Yg.sum(axis=0).plot.bar(figsize=(10,5))

"""# PRE PROCESSING"""

#list(enumerate(X.columns.tolist()))

"""## Filtro por bairro"""

Y = Y[bairros[0:]]

"""## Features

### Limpeza de dados constantes
"""

X = X.drop('latitude',axis=1).drop('longitude',axis=1).drop('tipo_y',axis=1)
#X.swaplevel(axis=1).sort_index(axis=1)

"""### Ordenar dados por equipamento"""

X = X.swaplevel(axis=1).sort_index(axis=1)

"""### Dia da semana e Mês"""

#pd.to_datetime(X.index.get_level_values(0))
#X['dia_semana'] = pd.to_datetime(X.index.get_level_values(0)).weekday
#X['mes'] = pd.to_datetime(X.index.get_level_values(0)).month

"""## Normalizar dados de qtd de veículos"""

X.columns[0:682]

scaler = preprocessing.StandardScaler()
min_max_scaler = preprocessing.MinMaxScaler()

"""### Padronização"""

X[X.columns[0:682]] = scaler.fit_transform(X[X.columns[0:682]])

"""### Min-Max"""

#X[X.columns[0:682]] = min_max_scaler.fit_transform(X[X.columns[0:682]])

"""## Imputar dados ausentes

### Moda
"""

imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

"""### Média a partir da padronização"""

X.fillna(0, inplace=True)

"""### Forward Fill + Backward Fill"""

#X = X.fillna(method='ffill', axis=0)

#X = X.fillna(method='bfill', axis=0)

"""## Ajuste temporal

* Excluir fevereiro, que fica isolado pela quebra na série temporal dos dados
"""

Xt = {}
Xt[2020] = X[:366*24]
Xt[2021] = X[-91*24:]
Yt = {}
Yt[2020] = Y[:366*24]
Yt[2021] = Y[-91*24:]

print('input 2020:',Xt[2020].shape)
print('output 2020:',Yt[2020].shape)
print('input 2021:',Xt[2021].shape)
print('output 2021:',Yt[2021].shape)

pca = PCA(n_components=2)
pca.fit(X)

Yt[2020].sum(axis=0).sort_values()

acidentes_bv = Yt[2020].iloc[60*24:].loc[Yt[2020].iloc[60*24:]['BOA VIAGEM'] == 1]['BOA VIAGEM']
transito_bv = Xt[2020].iloc[60*24:].loc[Yt[2020].iloc[60*24:]['BOA VIAGEM'] == 1]

acd_bv = pd.DataFrame(pca.transform(Xt[2020].iloc[60*24:].loc[Yt[2020].iloc[60*24:]['BOA VIAGEM'] == 1])).join(pd.Series(np.array(Yt[2020].iloc[60*24:].loc[Yt[2020].iloc[60*24:]['BOA VIAGEM'] == 1]['BOA VIAGEM']),name='acidentes'))
nacd_bv = pd.DataFrame(pca.transform(Xt[2020].iloc[60*24:].loc[Yt[2020].iloc[60*24:]['BOA VIAGEM'] == 0])).join(pd.Series(np.array(Yt[2020].iloc[60*24:].loc[Yt[2020].iloc[60*24:]['BOA VIAGEM'] == 0]['BOA VIAGEM']),name='acidentes'))
#acd_bv = pd.DataFrame(pca.transform(Xt[2020].loc[Yt[2020]['BOA VIAGEM'] == 1])).join(pd.Series(np.array(Yt[2020].loc[Yt[2020]['BOA VIAGEM'] == 1]['BOA VIAGEM']),name='acidentes'))
#nacd_bv = pd.DataFrame(pca.transform(Xt[2020].loc[Yt[2020]['BOA VIAGEM'] == 0])).join(pd.Series(np.array(Yt[2020].loc[Yt[2020]['BOA VIAGEM'] == 0]['BOA VIAGEM']),name='acidentes'))
jacd_bv = acd_bv.append(nacd_bv)

#acd_bv = pd.DataFrame(pca.transform(Xt[2020].loc[Yt[2020]['IMBIRIBEIRA'] == 1])).join(pd.Series(np.array(Yt[2020].loc[Yt[2020]['IMBIRIBEIRA'] == 1]['IMBIRIBEIRA']),name='acidentes'))
#nacd_bv = pd.DataFrame(pca.transform(Xt[2020].loc[Yt[2020]['IMBIRIBEIRA'] == 0])).join(pd.Series(np.array(Yt[2020].loc[Yt[2020]['IMBIRIBEIRA'] == 0]['IMBIRIBEIRA']),name='acidentes'))
#jacd_bv = acd_bv.append(nacd_bv)

fig, ax = plt.subplots(figsize=(10,10))
sns.scatterplot(data=jacd_bv.sort_values('acidentes'), x=0, y=1, hue='acidentes', size='acidentes')

acd_bv = pd.DataFrame(pca.transform(Xt[2021].loc[Yt[2021]['IMBIRIBEIRA'] == 1])).join(pd.Series(np.array(Yt[2021].loc[Yt[2021]['IMBIRIBEIRA'] == 1]['IMBIRIBEIRA']),name='acidentes'))
nacd_bv = pd.DataFrame(pca.transform(Xt[2021].loc[Yt[2021]['IMBIRIBEIRA'] == 0])).join(pd.Series(np.array(Yt[2021].loc[Yt[2021]['IMBIRIBEIRA'] == 0]['IMBIRIBEIRA']),name='acidentes'))
jacd_bv = acd_bv.append(nacd_bv)

fig, ax = plt.subplots(figsize=(10,10))
sns.scatterplot(data=jacd_bv.sort_values('acidentes'), x=0, y=1, hue='acidentes', size='acidentes')

sns.jointplot(data=acd_bv.sort_values('acidentes'), x=0, y=1, kind='hex')

sns.jointplot(data=acd_bv.sort_values('acidentes'), x=0, y=1, kind='hex')

sns.jointplot(data=nacd_bv.sort_values('acidentes'), x=0, y=1, kind='hex')

sns.jointplot(data=nacd_bv.sort_values('acidentes'), x=0, y=1, kind='hex')

"""# Janelas de tempo (tutorial)"""

train_df = Xt[2020]
val_df = Xt[2021][0:60]
test_df = Xt[2021][60:]

class WindowGenerator():
  def __init__(self, input_width, label_width, shift,
               train_df=train_df, val_df=val_df, test_df=test_df,
               label_columns=None):
    # Store the raw data.
    self.train_df = train_df
    self.val_df = val_df
    self.test_df = test_df

    # Work out the label column indices.
    self.label_columns = label_columns
    if label_columns is not None:
      self.label_columns_indices = {name: i for i, name in
                                    enumerate(label_columns)}
    self.column_indices = {name: i for i, name in
                           enumerate(train_df.columns)}

    # Work out the window parameters.
    self.input_width = input_width
    self.label_width = label_width
    self.shift = shift

    self.total_window_size = input_width + shift

    self.input_slice = slice(0, input_width)
    self.input_indices = np.arange(self.total_window_size)[self.input_slice]

    self.label_start = self.total_window_size - self.label_width
    self.labels_slice = slice(self.label_start, None)
    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

  def __repr__(self):
    return '\n'.join([
        f'Total window size: {self.total_window_size}',
        f'Input indices: {self.input_indices}',
        f'Label indices: {self.label_indices}',
        f'Label column name(s): {self.label_columns}'])

def split_window(self, features):
  inputs = features[:, self.input_slice, :]
  labels = features[:, self.labels_slice, :]
  if self.label_columns is not None:
    labels = tf.stack(
        [labels[:, :, self.column_indices[name]] for name in self.label_columns],
        axis=-1)

  # Slicing doesn't preserve static shape information, so set the shapes
  # manually. This way the `tf.data.Datasets` are easier to inspect.
  inputs.set_shape([None, self.input_width, None])
  labels.set_shape([None, self.label_width, None])

  return inputs, labels

WindowGenerator.split_window = split_window

def plot(self, model=None, plot_col='acidentes', max_subplots=3):
  inputs, labels = self.example
  plt.figure(figsize=(12, 8))
  plot_col_index = self.column_indices[plot_col]
  max_n = min(max_subplots, len(inputs))
  for n in range(max_n):
    plt.subplot(max_n, 1, n+1)
    plt.ylabel(f'{plot_col} [normed]')
    plt.plot(self.input_indices, inputs[n, :, plot_col_index],
             label='Inputs', marker='.', zorder=-10)

    if self.label_columns:
      label_col_index = self.label_columns_indices.get(plot_col, None)
    else:
      label_col_index = plot_col_index

    if label_col_index is None:
      continue

    plt.scatter(self.label_indices, labels[n, :, label_col_index],
                edgecolors='k', label='Labels', c='#2ca02c', s=64)
    if model is not None:
      predictions = model(inputs)
      plt.scatter(self.label_indices, predictions[n, :, label_col_index],
                  marker='X', edgecolors='k', label='Predictions',
                  c='#ff7f0e', s=64)

    if n == 0:
      plt.legend()

  plt.xlabel('Time [h]')

WindowGenerator.plot = plot

"""# PCA e correlações"""

pca = PCA(n_components=2)

pca.fit(Xt[2020])

print(pca.explained_variance_ratio_)

pd.DataFrame(pca.fit_transform(Xt[2020])).plot.scatter(x=0,y=1)

Yt[2020].max(axis=1)

fig, ax = plt.subplots(figsize=(10,10))
df = pd.DataFrame(pca.fit_transform(Xt[2020])).join(pd.Series(np.array(Yt[2020].max(axis=1)), name='acidentes'))
sns.scatterplot(data=df.sort_values('acidentes'), x=0, y=1, hue='acidentes', size='acidentes')

pca.fit(Xt[2020])

Xorg = pd.read_csv('https://www.cin.ufpe.br/~amal/dados/transito/DATAMART/inputhgeralpivot.csv', index_col=[0,1], header=[0,1])
Xorg = Xorg[Xorg.columns[0:682]]

Xorg.groupby('equipamento', level=1, axis=1).sum().corrwith(Y.sum(axis=1)).plot(figsize=(10,5))

pd.DataFrame(X[X.columns[0:682]].groupby('equipamento',level=1,axis=1).sum()).corrwith(Y.sum(axis=1)).plot(figsize=(10,5))

Xorg2020 = Xorg[:366*24]

imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
 Xorg2020 = imp.fit_transform(Xorg2020)

print('lag 0: ', pd.DataFrame(np.roll(np.array(Xorg2020),0, axis=0).sum(axis=1)).corrwith(Yt[2020].sum(axis=1).reset_index()[0]))
print('lag 1: ', pd.DataFrame(np.roll(np.array(Xorg2020),-1, axis=0).sum(axis=1)).corrwith(Yt[2020].sum(axis=1).reset_index()[0]))
print('lag 2: ', pd.DataFrame(np.roll(np.array(Xorg2020),-2, axis=0).sum(axis=1)).corrwith(Yt[2020].sum(axis=1).reset_index()[0]))
print('lag 3: ', pd.DataFrame(np.roll(np.array(Xorg2020),-3, axis=0).sum(axis=1)).corrwith(Yt[2020].sum(axis=1).reset_index()[0]))
print('lag 4: ', pd.DataFrame(np.roll(np.array(Xorg2020),-5, axis=0).sum(axis=1)).corrwith(Yt[2020].sum(axis=1).reset_index()[0]))
print('lag 5: ', pd.DataFrame(np.roll(np.array(Xorg2020),-5, axis=0).sum(axis=1)).corrwith(Yt[2020].sum(axis=1).reset_index()[0]))
print('lag 6: ', pd.DataFrame(np.roll(np.array(Xorg2020),-6, axis=0).sum(axis=1)).corrwith(Yt[2020].sum(axis=1).reset_index()[0]))

Y

"""# Train/Test Split

* Criação do tensor de entrada com 6 lags/timesteps
  - Treino/validação
  - Teste
"""

len(Xt[2020]) + len(Xt[2021])

lags = 12
npX = []
for lag in range(lags):
  npX.insert(0, np.roll(np.array(Xt[2020]), lag * -1, axis=0))
npX = np.array(npX)
npX = npX.reshape(npX.shape[1],npX.shape[2],npX.shape[0])[0:-lags]

Xt[2020].shape

npX.shape

npXtest = []
for lag in range(lags):
  npXtest.insert(0, np.roll(np.array(Xt[2021]), lag * -1, axis=0))
npXtest = np.array(npXtest)
npXtest = npXtest.reshape(npXtest.shape[1],npXtest.shape[2],npXtest.shape[0])[0:-lags]

npXtest.shape

"""* Criação da matriz de saída descontando os lags iniciais
  - Treino/validação
  - Teste
"""

Xt[2020]

"""### Saída com janela deslizante"""

Yt[2020].head(10)

lags = lags
indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=lags)
npY = np.array(Yt[2020].rolling(window=indexer).max())[:-lags]
npYtest = np.array(Yt[2021].rolling(window=indexer).max())[:-lags:]

dir(pd.api.indexers)

npY[:, 0][lags:][:20]

npY[:, 18][lags:][:20]

print('acidentes por hora treino\n', pd.Series(npY.flatten()).value_counts(), '\n percentuais \n', pd.Series(npY.flatten()).value_counts()*100/pd.Series(npY.flatten()).value_counts().sum())
print('acidentes por hora teste\n', pd.Series(npYtest.flatten()).value_counts(), '\n percentuais \n', pd.Series(npYtest.flatten()).value_counts()*100/pd.Series(npYtest.flatten()).value_counts().sum())

Yt[2020].rolling(window=lag).max().melt()['value'].value_counts()

Yt[2020].rolling(window=lag).max().melt()['value'].value_counts()*100/Yt[2020].rolling(window=3).max().melt()['value'].value_counts().sum()

#npX = np.concatenate((npX,npXtest[:60*24]),axis=0)
#npXtest = npXtest[60*24:]

#npY = np.concatenate((npY,npYtest[:60*24]),axis=0)
#npYtest = npYtest[60*24:]

print(npX.shape)
print(npXtest.shape)
print(npY.shape)
print(npYtest.shape)

"""* Criação dos folds da validação cruzada: treinamento/validação"""

tscv = TimeSeriesSplit()
print(tscv)
k = 0
folds = list(range(5))

for train_index, test_index in tscv.split(npX):
    folds[k] = [train_index, test_index]
    k = k + 1
    #print("TRAIN:", train_index, "TEST:", test_index)
    #X_train, X_test = Xt[2020].reset_index().loc[train_index], Xt[2020].reset_index().loc[test_index]
    #y_train, y_test = Yt[2020].reset_index().loc[train_index], Yt[2020].reset_index().loc[test_index]

"""# MODELS

## Baseline
"""

pred_base = np.zeros(npYtest.shape)

accuracy_score(pd.Series(pred_base.flatten()), pd.Series(npYtest[:,:].flatten()))

precision_score(pd.Series(pred_base.flatten()), pd.Series(npYtest[:,:].flatten()))

recall_score(pd.Series(pred_base.flatten()), pd.Series(npYtest[:,:].flatten()))

f1_score(pd.Series(pred_base.flatten()), pd.Series(npYtest[:,:].flatten()))

"""## Setup"""

logdir = pathlib.Path(tempfile.mkdtemp())/"tensorboard_logs"
shutil.rmtree(logdir, ignore_errors=True)

def plot_cm(labels, predictions, p=0.5):
  cm = confusion_matrix(labels, predictions > p)
  plt.figure(figsize=(5,5))
  sns.heatmap(cm, annot=True, fmt="d")
  plt.title('Confusion matrix @{:.2f}'.format(p))
  plt.ylabel('Actual label')
  plt.xlabel('Predicted label')

  print('Sem acidente previsto e ocorrido (True Negatives): ', cm[0][0])
  print('Com acidente previsto, mas não ocorrido (False Positives): ', cm[0][1])
  print('Sem acidente previsto, mas ocorrido (False Negatives): ', cm[1][0])
  print('Com acidente previsto e ocorrido (True Positives): ', cm[1][1])
  print('Total de acidentes: ', np.sum(cm[1]))

colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
def plot_metrics(history):
  metrics = ['loss', 'prc', 'precision', 'recall']
  plt.subplots(figsize=(8,8))
  for n, metric in enumerate(metrics):
    name = metric.replace("_"," ").capitalize()
    plt.subplot(2,2,n+1)
    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')
    plt.plot(history.epoch, history.history['val_'+metric],
             color=colors[0], linestyle="--", label='Val')
    plt.xlabel('Epoch')
    plt.ylabel(name)
    if metric == 'loss':
      plt.ylim([0, plt.ylim()[1]])
    elif metric == 'auc':
      plt.ylim([0.8,1])
    else:
      plt.ylim([0,1])

    plt.legend()

def plot_loss(history, label, n):
  # Use a log scale on y-axis to show the wide range of values.
  plt.semilogy(history.epoch, history.history['loss'],
               color=colors[n], label='Train ' + label)
  plt.semilogy(history.epoch, history.history['val_loss'],
               color=colors[n], label='Val ' + label,
               linestyle="--")
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.legend()

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
]

# Scaling by total/2 helps keep the loss to a similar magnitude.
# The sum of the weights of all examples stays the same.
pos = (npY.shape[0] - npY[:,2].sum())
neg = npY[:,2].sum()

weight_for_0 = (1 / (npY.shape[0] - npY[:,2].sum())) * (npY.shape[0] / 2.0)
weight_for_1 = (1 / npY[:,2].sum()) * (npY.shape[0] / 2.0)

class_weight = {0: weight_for_0, 1: weight_for_1}

print('Weight for class 0: {:.2f}'.format(weight_for_0))
print('Weight for class 1: {:.2f}'.format(weight_for_1))

npX2 = npX.reshape(-1, npX.shape[2], npX.shape[1])
npX2test = npXtest.reshape(-1, npXtest.shape[2], npXtest.shape[1])


npX3 = npX.reshape(-1, npX.shape[2], npX.shape[1])
npX3est = npXtest.reshape(-1, npXtest.shape[2], npXtest.shape[1])

npX4 = npX.reshape(-1, npX.shape[2], 11, 62)
npX4test = npXtest.reshape(-1, npXtest.shape[2], 11, 62)
npX4test.shape

def convert(x):
  return 1 if x > 0.5 else 0

N_TRAIN = npX.shape[0]
BATCH_SIZE = 240
STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE

lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
  0.001,
  decay_steps=STEPS_PER_EPOCH*10,
  decay_rate=1,
  staircase=False)

def get_optimizer():
  return tf.keras.optimizers.Adam(lr_schedule)

"""## Dense

### Todos
"""

model2 = keras.Sequential([
    keras.Input(shape=(npX.shape[1], npX.shape[2])),
    #layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    #keras.layers.Dropout(0.5),
    #layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    #layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),
    #layers.Dense(256, activation='relu'),
    #keras.layers.Dropout(0.5),
    #layers.Dense(512, activation='relu'),
    #keras.layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.5),
    #layers.Dense(128, activation='relu'),
    #keras.layers.Dropout(0.2),
    #layers.Dense(128, activation='relu'),
    #keras.layers.Dropout(0.2),
    #layers.Dense(128, activation='relu'),
    #keras.layers.Dropout(0.2),
    #layers.Dense(128, activation='relu'),
    layers.Flatten(),
    #layers.Dense(1, activation='sigmoid'),
    layers.Dense(npY.shape[1], activation='sigmoid'),
])

model2.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def get_callbacks(name):
#   return [ tf.keras.callbacks.EarlyStopping(monitor='val_prc', patience=3), tf.keras.callbacks.TensorBoard(logdir/name)]
# 
# model2.compile(
#     loss=keras.losses.BinaryCrossentropy(from_logits=False),
#     #optimizer=keras.optimizers.Adam(learning_rate=1e-3),
#     optimizer=get_optimizer(),
#     metrics=[METRICS],
# )
# 
# history_mlpt = model2.fit(
#     npX, npY[:,:], validation_data=(npXtest, npYtest[:,:]), batch_size=BATCH_SIZE, epochs=30, callbacks=get_callbacks('MLP')
# )
# plot_metrics(history_mlpt)

pred_mlp2 = model2.predict(npXtest[:,:,:])

accuracy_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

precision_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

recall_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

f1_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

results = {}
pred_mlp2 = model2.predict(npXtest[:,:,:])
for k, b in enumerate(pred_mlp2.T):
  results[bairros[k]] = {}
  results[bairros[k]]['acuracia'] = accuracy_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))
  results[bairros[k]]['precisao'] = precision_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))
  results[bairros[k]]['cobertura'] = recall_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))
  results[bairros[k]]['f1'] = f1_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))

resultado = pd.DataFrame(results).T


with pd.ExcelWriter(path,mode='a') as writer:
    resultado.to_excel(writer, sheet_name='mlp')

"""## LSTM

### Todos
"""

model = keras.Sequential([
    keras.Input(shape=(npX.shape[2], npX.shape[1])),
    layers.LSTM(128, return_sequences=True),
    keras.layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.5),
    #layers.GRU(512, return_sequences=True),
    layers.Flatten(),
    layers.Dense(npY.shape[1], activation='sigmoid'),
])

model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def get_callbacks(name):
#   return [ tf.keras.callbacks.EarlyStopping(monitor='val_prc', patience=3), tf.keras.callbacks.TensorBoard(logdir/name)]
# 
# 
# model.compile(
#     loss=keras.losses.BinaryCrossentropy(from_logits=False),
#     optimizer=get_optimizer(),
#     #optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
#     metrics=[METRICS],
# )
# 
# 
# history_lstm = model.fit(
#     npX2, npY[:,:], validation_data=(npX2test, npYtest[:,:]), batch_size=BATCH_SIZE, epochs=20, callbacks=get_callbacks('LSTM')
# )
# plot_metrics(history_lstm)

pred_mlp2 = model2.predict(npXtest[:,:,:])

accuracy_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

precision_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

recall_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

f1_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

results = {}
pred_mlp2 = model.predict(npX2test[:,:,:])
for k, b in enumerate(pred_mlp2.T):
  results[bairros[k]] = {}
  results[bairros[k]]['acuracia'] = accuracy_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))
  results[bairros[k]]['precisao'] = precision_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))
  results[bairros[k]]['cobertura'] = recall_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))
  results[bairros[k]]['f1'] = f1_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))

resultado = pd.DataFrame(results).T


with pd.ExcelWriter(path,mode='a') as writer:
    resultado.to_excel(writer, sheet_name='lstm')

"""### Res LSTM

"""

class ResidualWrapper(tf.keras.Model):
  def __init__(self, model):
    super().__init__()
    self.model = model

  def call(self, inputs, *args, **kwargs):
    delta = self.model(inputs, *args, **kwargs)

    # The prediction for each time step is the input
    # from the previous time step plus the delta
    # calculated by the model.
    return inputs + delta

# Commented out IPython magic to ensure Python compatibility.
# %%time
# """
# residual_lstm = ResidualWrapper(
#     tf.keras.Sequential([
#     tf.keras.layers.LSTM(32, return_sequences=True),
#     tf.keras.layers.Dense(
#         num_features,
#         # The predicted deltas should start small.
#         # Therefore, initialize the output layer with zeros.
#         kernel_initializer=tf.initializers.zeros())
# ]))
# 
# history = compile_and_fit(residual_lstm, wide_window)
# 
# IPython.display.clear_output()
# val_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val)
# performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test, verbose=0)
# print()
# """

model = ResidualWrapper(
    keras.Sequential([
    keras.Input(shape=(npX.shape[2], npX.shape[1])),
    layers.LSTM(512, return_sequences=True),
    #layers.GRU(512, return_sequences=True),
    layers.Flatten(),
    layers.Dense(npY.shape[1], activation='sigmoid', kernel_initializer=tf.initializers.zeros()),
  ])
)

# não funciona
"""
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

model.compile(
    loss=keras.losses.BinaryCrossentropy(),    
    #optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),
    metrics=[METRICS],
)


history_reslstm = model.fit(
    npX2, npY[:,:], validation_data=(npX2test, npYtest[:,:]), batch_size=1024, epochs=50
)
"""

"""# CNN"""

CONV_WIDTH = 4

model = keras.Sequential([
    keras.Input(shape=(npX.shape[2], npX.shape[1])),
    #layers.LSTM(512),
    layers.Conv1D(filters=64, kernel_size=(CONV_WIDTH,), activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    #layers.Dense(512, activation='relu'),
    #layers.Dense(512, activation='relu'),
    #layers.Dense(512, activation='relu'),
    #layers.Dense(npY.shape[1], activation='softmax'),
    layers.Flatten(),
    layers.Dense(npY.shape[1], activation='sigmoid'),
])

model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# def get_callbacks(name):
#   return [ tf.keras.callbacks.EarlyStopping(monitor='val_prc', patience=3), tf.keras.callbacks.TensorBoard(logdir/name)]
# 
# 
# model.compile(
#     #loss=keras.losses.MeanAbsoluteError(),
#     loss=keras.losses.BinaryCrossentropy(from_logits=False),
#     optimizer=get_optimizer(),    
#     #optimizer=keras.optimizers.Adam(learning_rate=1e-3),
#     metrics=[METRICS],
# )
# 
# 
# history_cnn = model.fit(
#     npX2, npY[:,:], validation_data=(npX2test, npYtest[:,:]), batch_size=BATCH_SIZE, epochs=20, callbacks=get_callbacks('LSTM')
# )
# 
# plot_metrics(history_cnn)

pred_mlp2 = model.predict(npX2test[:,:,:])

accuracy_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,].flatten()))

recall_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,].flatten()))

f1_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,].flatten()))

f1_score(pd.Series(pred_mlp2.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

results = {}
pred_mlp2 = model.predict(npX2test[:,:,:])
for k, b in enumerate(pred_mlp2.T):
  results[bairros[k]] = {}
  results[bairros[k]]['acuracia'] = accuracy_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))
  results[bairros[k]]['precisao'] = precision_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))
  results[bairros[k]]['cobertura'] = recall_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))
  results[bairros[k]]['f1'] = f1_score(npYtest[:,k], pd.Series(pred_mlp2[:,k].flatten()).map(convert))

resultado = pd.DataFrame(results).T


with pd.ExcelWriter(path,mode='a') as writer:
    resultado.to_excel(writer, sheet_name='cnn')

"""## CNN +  LSTM"""

CONV_WIDTH = 4

model = keras.Sequential([
    keras.Input(shape=(npX4.shape[1], npX4.shape[2], npX4.shape[3])),
    #layers.Conv1D(filters=32, kernel_size=(CONV_WIDTH,), activation='relu'),
    #layers.GRU(512),
    layers.ConvLSTM1D(filters=64, kernel_size=(CONV_WIDTH,), activation='relu'),
    layers.MaxPooling1D(pool_size=2, strides=1, padding='valid'),
    layers.Dropout(0.5),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    #layers.Dense(512, activation='relu'),
    #layers.Dense(512, activation='relu'),
    #layers.Dense(npY.shape[1], activation='softmax'),
    layers.Flatten(),
    layers.Dense(npY.shape[1], activation='sigmoid'),
])

model.summary()

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

model.compile(
    #loss=keras.losses.MeanAbsoluteError(),
    loss=keras.losses.BinaryCrossentropy(from_logits=False),
    optimizer=get_optimizer(), 
    #optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    metrics=[METRICS],
)


history_cnnlstm = model.fit(
    npX4, npY[:,:], validation_data=(npX4test, npYtest[:,:]), batch_size=BATCH_SIZE, epochs=20, callbacks=get_callbacks('LSTM')
)
 
plot_metrics(history_cnnlstm)

pred_mlp4 = model.predict(npX4test[:,:,:])

accuracy_score(pd.Series(pred_mlp4.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

precision_score(pd.Series(pred_mlp4.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

recall_score(pd.Series(pred_mlp4.flatten()).map(convert), pd.Series(npYtest[:,:].flatten()))

f1_score(pd.Series(pred_mlp4.flatten()).map(convert), pd.Series(npYtest[:,].flatten()))

results = {}
pred_mlp4 = model.predict(npX4test[:,:,:])
for k, b in enumerate(pred_mlp2.T):
  results[bairros[k]] = {}
  results[bairros[k]]['acuracia'] = accuracy_score(npYtest[:,k], pd.Series(pred_mlp4[:,k].flatten()).map(convert))
  results[bairros[k]]['precisao'] = precision_score(npYtest[:,k], pd.Series(pred_mlp4[:,k].flatten()).map(convert))
  results[bairros[k]]['cobertura'] = recall_score(npYtest[:,k], pd.Series(pred_mlp4[:,k].flatten()).map(convert))
  results[bairros[k]]['f1'] = f1_score(npYtest[:,k], pd.Series(pred_mlp4[:,k].flatten()).map(convert))

resultado = pd.DataFrame(results).T


with pd.ExcelWriter(path,mode='a') as writer:
    resultado.to_excel(writer, sheet_name='cnnlstm')

resultado

plot_cm(pd.Series(npYtest[:,].flatten()), pd.Series(pred_mlp4.flatten()))